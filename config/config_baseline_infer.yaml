################# Training Control #################
deterministic: False
use_compile: True
seed: 1234
lr: 1e-3
max_epochs: 40
lr_scheduler: step
lr_decay_epochs: 4
lr_decay_rate: 0.5
lr_decay_min_lr: 1e-6
infer_stage: True

################# Distributed Training Control #################
devices: 1
num_nodes: 1
strategy: ddp

################# Dataset Setting #################
dataset_class_name: coco
dataset_dir: data
split_ratios: [0.8, 0.1, 0.1]
batch_size: 32
test_batch_size: 32
num_workers: 8
persistent_workers: True
img_dir: data/coco/images
ann_dir: data/coco/annotations

################# Model Architecture, Optimizers and Loss Functions #################
# Basic setting
model_class_name: baseline
weight_decay: 1e-6

# VQVAE model setting
pretrained_vqvae_path: //fs/nexus-scratch/tuxunlu/git/CMSC848M-final-project/vqvae/vqvae_4800/version_0/checkpoints/best-epoch=004-train_loss=0.128738.ckpt
pretrained_transformer_path: /fs/nexus-scratch/tuxunlu/git/CMSC848M-final-project/transformer/version_0/checkpoints/best-epoch=000-train_loss=0.000053.ckpt
h_dim: 128
res_h_dim: 128
n_res_layers: 4
n_embeddings: 1024
embedding_dim: 128
beta: 0.25
save_img_embedding_map: False

# Transformer model setting
src_vocab_size: 49408 # 49406 + <start>/<end> token. <start> token id is 49406, <end> token id is 49407
tgt_vocab_size: 1026 # 1024 + <start>/<end> tokens
d_model: 512
num_heads: 8
num_encoder_layers: 2
num_decoder_layers: 2
dim_ff: 512
dropout: 0.1
input_max_len: 256
output_max_len: 1200

################# Tensorboard Logger Setting #################
log_dir: 'lightning_logs'
experiment_name: 'main'

################# Checkpoint & Restart Control #################
enable_checkpointing: True