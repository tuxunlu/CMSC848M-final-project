################# Training Control #################
deterministic: False
use_compile: True
seed: 1234
lr: 1e-3
max_epochs: 40
lr_scheduler: step
lr_decay_epochs: 4
lr_decay_rate: 0.5
lr_decay_min_lr: 1e-6
infer_stage: False

################# Distributed Training Control #################
devices: 4
num_nodes: 1
strategy: ddp

################# Dataset Setting #################
dataset_class_name: coco
dataset_dir: dataset
split_ratios: [0.8, 0.1, 0.1]
batch_size: 32
test_batch_size: 32
num_workers: 8
persistent_workers: True
img_dir: data/coco/images
ann_dir: data/coco/annotations

################# Model Architecture, Optimizers and Loss Functions #################
# Basic setting
model_class_name: baseline
weight_decay: 1e-6

# VQVAE model setting
pretrained_vqvae_path: lightning_logs/vqvae_4800/version_0/checkpoints/last.ckpt
pretrained_transformer_path: None
h_dim: 128
res_h_dim: 128
n_res_layers: 4
n_embeddings: 1024
embedding_dim: 128
downsample_height: 2
downsample_width: 2
beta: 0.25
save_img_embedding_map: False

# Transformer model setting
src_vocab_size: 49408 # 49406 + <start>/<end> token. <start> token id is 49406, <end> token id is 49407
tgt_vocab_size: 1025 # 1024 + <start> token
d_model: 512
num_heads: 8
num_encoder_layers: 2
num_decoder_layers: 2
dim_ff: 512
dropout: 0.1
input_max_len: 256
output_max_len: 4801
tgt_start_token_id: 1024

################# Tensorboard Logger Setting #################
log_dir: 'lightning_logs'
experiment_name: 'main'

################# Checkpoint & Restart Control #################
enable_checkpointing: True